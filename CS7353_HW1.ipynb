{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "欢迎来到上海交通大学 CS7353《[设计和理解深度神经网络](https://cs7353.netlify.app/)》！\n",
        "\n",
        "这里是第一次课程作业，具体时间信息见[课程网站](https://cs7353.netlify.app/)。作业在 Canvas 上提交，注意时间节点。只需要上传一份 ipynb 文件，请务必保留每个单元格的运行结果。\n",
        "\n",
        "如有任何问题，请联系[助教](https://cs7353.netlify.app/staff/)。\n",
        "\n",
        "# 1 简介\n",
        "\n",
        "在本次作业中，您将练习编写前向传播和反向传播代码。本次作业不需要用到GPU。\n",
        "\n",
        "此任务的目标如下：\n",
        "\n",
        "- 了解并能够实现（矢量化的）**反向传播**\n",
        "- 为深度网络实现**最大池化**\n",
        "- 为深度网络实现**批量归一化**\n",
        "- 为深度网络实现**卷积**\n",
        "\n",
        "注意，请严格遵守以下注意事项，如有违背，本次作业零分处理：\n",
        "- 请仅在标明的 TODO 位置完成代码，请勿更改其他代码；\n",
        "- 请勿 import 其他 python package（补充：请手搓，不要为了省事直接调用打包好的函数）；\n",
        "- 请务必保留每个单元格的运行结果。"
      ],
      "metadata": {
        "id": "nP73M5d0QiXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 准备：辅助函数\n",
        "\n",
        "这里是用于检测结果的辅助函数，**您不必进行任何代码层面的操作**，仅需运行对应单元格。"
      ],
      "metadata": {
        "id": "-iLUHDIfabrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "seed = 7\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "\n",
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
        "\n",
        "\n",
        "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
        "    \"\"\"\n",
        "    Evaluate a numeric gradient for a function that accepts a numpy\n",
        "    array and returns a numpy array.\n",
        "    \"\"\"\n",
        "    grad = np.zeros_like(x)\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h\n",
        "        pos = f(x).copy()\n",
        "        x[ix] = oldval - h\n",
        "        neg = f(x).copy()\n",
        "        x[ix] = oldval\n",
        "\n",
        "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
        "        it.iternext()\n",
        "    return grad"
      ],
      "metadata": {
        "id": "7BpuEnwWNeMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 卷积（Convolution）\n",
        "\n",
        "卷积是深度学习中一种重要的操作，尤其在卷积神经网络中广泛应用。它主要用于从输入数据中提取特征。\n",
        "\n",
        "卷积操作的基本思想是通过滑动一个称为卷积核（或滤波器）的小窗口，对输入数据进行局部区域的加权求和。这个卷积核在整个输入数据上滑动，每一次都产生一个输出值。通过调整卷积核的参数，网络可以学习到不同的特征，例如边缘、纹理、或更高级的抽象特征。\n",
        "\n",
        "卷积操作的优势在于它能够有效地捕捉输入数据中的局部特征，而且参数共享的机制使得网络对于不同位置的相似特征具有更好的学习能力。此外，卷积操作也具有降维的作用，可以减少模型的参数数量，有助于提高计算效率。\n",
        "\n",
        "在卷积神经网络中，多个卷积层可以通过堆叠来提取更高级别的特征，构建复杂的特征层次结构，从而更好地适应不同的任务，如图像分类、物体检测等。"
      ],
      "metadata": {
        "id": "o9z-PRJNcG__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 卷积：朴素前向传播\n",
        "卷积神经网络的核心是卷积操作。在函数`conv_forward_naive`中实现卷积层的前向传播。\n",
        "\n",
        "在此阶段，您不必过于担心效率；只需以您认为最清晰的方式编写代码即可。"
      ],
      "metadata": {
        "id": "zMYPMMAxY-2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_forward_naive(x, w, b, conv_param):\n",
        "    \"\"\"\n",
        "    A naive implementation of the forward pass for a convolutional layer.\n",
        "\n",
        "    The input consists of N data points, each with C channels, height H and width\n",
        "    W. We convolve each input with F different filters, where each filter spans\n",
        "    all C channels and has height HH and width HH.\n",
        "\n",
        "    Input:\n",
        "    - x: Input data of shape (N, C, H, W)\n",
        "    - w: Filter weights of shape (F, C, HH, WW)\n",
        "    - b: Biases, of shape (F,)\n",
        "    - conv_param: A dictionary with the following keys:\n",
        "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
        "        horizontal and vertical directions.\n",
        "      - 'pad': The number of pixels that will be used to zero-pad the input.\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
        "      H' = 1 + (H + 2 * pad - HH) / stride\n",
        "      W' = 1 + (W + 2 * pad - WW) / stride\n",
        "    - cache: (x, w, b, conv_param)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    #############################################################################\n",
        "    # TODO: Implement the convolutional forward pass.                           #\n",
        "    # Hint: you can use the function np.pad for padding.                        #\n",
        "    #############################################################################\n",
        "    stride = conv_param['stride']\n",
        "    pad = conv_param['pad']\n",
        "\n",
        "    N, C, H, W = x.shape\n",
        "    F, _, HH, WW = w.shape\n",
        "\n",
        "    H_out = 1 + (H + 2 * pad - HH) // stride\n",
        "    W_out = 1 + (W + 2 * pad - WW) // stride\n",
        "\n",
        "    x_padded = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant', constant_values=0)\n",
        "\n",
        "    out = np.zeros((N, F, H_out, W_out))\n",
        "\n",
        "    for n in range(N):\n",
        "      for f in range(F):\n",
        "        for i in range(H_out):\n",
        "          for j in range(W_out):\n",
        "            h_start = i*stride\n",
        "            h_end = h_start + HH\n",
        "            w_start = j*stride\n",
        "            w_end = w_start + WW\n",
        "\n",
        "            region = x_padded[n, :, h_start:h_end, w_start:w_end]\n",
        "            out[n,f,i,j] = np.sum (region*w[f]) + b[f]\n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "    cache = (x, w, b, conv_param)\n",
        "    return out, cache"
      ],
      "metadata": {
        "id": "EpoTnPSTZWyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "您可以通过运行以下测试您的实现，您应该看到误差小于 ``1e-7``。"
      ],
      "metadata": {
        "id": "utGLyEEZZZlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_shape = (2, 3, 4, 4)\n",
        "w_shape = (3, 3, 4, 4)\n",
        "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
        "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
        "b = np.linspace(-0.1, 0.2, num=3)\n",
        "\n",
        "conv_param = {'stride': 2, 'pad': 1}\n",
        "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
        "correct_out = np.array([[[[[-0.08759809, -0.10987781],\n",
        "                           [-0.18387192, -0.2109216 ]],\n",
        "                          [[ 0.21027089,  0.21661097],\n",
        "                           [ 0.22847626,  0.23004637]],\n",
        "                          [[ 0.50813986,  0.54309974],\n",
        "                           [ 0.64082444,  0.67101435]]],\n",
        "                         [[[-0.98053589, -1.03143541],\n",
        "                           [-1.19128892, -1.24695841]],\n",
        "                          [[ 0.69108355,  0.66880383],\n",
        "                           [ 0.59480972,  0.56776003]],\n",
        "                          [[ 2.36270298,  2.36904306],\n",
        "                           [ 2.38090835,  2.38247847]]]]])\n",
        "\n",
        "# Compare your output to ours; difference should be around 1e-8\n",
        "print ('Testing conv_forward_naive')\n",
        "print ('conv forward error: ', rel_error(out, correct_out))"
      ],
      "metadata": {
        "id": "U0iaZH30Yp3L",
        "outputId": "7e1cf1a2-34ca-4dc9-932e-c1292fb7620d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing conv_forward_naive\n",
            "conv forward error:  2.2121476417505994e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 卷积：朴素反向传播\n",
        "在函数`conv_backward_naive`中实现卷积操作的反向传播。同样，您不必过于担心计算效率。"
      ],
      "metadata": {
        "id": "uw5sC0F8Z9nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_backward_naive(dout, cache):\n",
        "    \"\"\"\n",
        "    A naive implementation of the backward pass for a convolutional layer.\n",
        "\n",
        "    Inputs:\n",
        "    - dout: Upstream derivatives.\n",
        "    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x\n",
        "    - dw: Gradient with respect to w\n",
        "    - db: Gradient with respect to b\n",
        "    \"\"\"\n",
        "    dx, dw, db = None, None, None\n",
        "    #############################################################################\n",
        "    # TODO: Implement the convolutional backward pass.                          #\n",
        "    #############################################################################\n",
        "\n",
        "    x, w, b, conv_param = cache\n",
        "    stride = conv_param['stride']\n",
        "    pad = conv_param['pad']\n",
        "\n",
        "    N, C, H, W = x.shape\n",
        "    F, _, HH, WW = w.shape\n",
        "    _, _, H_out, W_out = dout.shape\n",
        "\n",
        "    # Initialize gradients\n",
        "    dx = np.zeros_like(x)\n",
        "    dw = np.zeros_like(w)\n",
        "    db = np.zeros_like(b)\n",
        "\n",
        "    # Pad x and dx\n",
        "    x_padded = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant', constant_values=0)\n",
        "    dx_padded = np.pad(dx, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant', constant_values=0)\n",
        "\n",
        "    # Compute db (sum over dout)\n",
        "    db = np.sum(dout, axis=(0, 2, 3))\n",
        "\n",
        "    for n in range(N):\n",
        "      for f in range(F):\n",
        "        for i in range(H_out):\n",
        "          for j in range(W_out):\n",
        "            h_start = i*stride\n",
        "            h_end = h_start + HH\n",
        "            w_start = j*stride\n",
        "            w_end = w_start + WW\n",
        "\n",
        "            region = x_padded[n, :, h_start:h_end, w_start:w_end]\n",
        "\n",
        "            dw[f] += region * dout[n, f, i, j]\n",
        "\n",
        "            dx_padded[n, :, h_start:h_end, w_start:w_end] += w[f] * dout[n, f, i, j]\n",
        "\n",
        "    dx = dx_padded[:, :, pad:-pad, pad:-pad] if pad > 0 else dx_padded\n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "    return dx, dw, db\n"
      ],
      "metadata": {
        "id": "vkuLPj0paBn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "完成后，运行以下内容以使用数值梯度检查验证您的反向传播，您应该看到误差小于 ``1e-8``。"
      ],
      "metadata": {
        "id": "NJTPMxxaaB_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.randn(4, 3, 5, 5)\n",
        "w = np.random.randn(2, 3, 3, 3)\n",
        "b = np.random.randn(2,)\n",
        "dout = np.random.randn(4, 2, 5, 5)\n",
        "conv_param = {'stride': 1, 'pad': 1}\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, conv_param)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, conv_param)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, conv_param)[0], b, dout)\n",
        "\n",
        "out, cache = conv_forward_naive(x, w, b, conv_param)\n",
        "dx, dw, db = conv_backward_naive(dout, cache)\n",
        "\n",
        "# Your errors should be around 1e-9'\n",
        "print ('Testing conv_backward_naive function')\n",
        "print ('dx error: ', rel_error(dx, dx_num))\n",
        "print ('dw error: ', rel_error(dw, dw_num))\n",
        "print ('db error: ', rel_error(db, db_num))"
      ],
      "metadata": {
        "id": "pDpcA42MaEHN",
        "outputId": "4e15e6dd-448a-443a-edde-a6885b5cf473",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing conv_backward_naive function\n",
            "dx error:  1.6877671244138885e-09\n",
            "dw error:  4.5472807201787456e-10\n",
            "db error:  8.919734826253235e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 最大池化（Max Pooling）\n",
        "\n",
        "最大池化是深度学习中常用的一种池化操作，用于降低输入数据的空间维度。在卷积神经网络中，它通常用于减小特征图的尺寸，从而减少模型的计算复杂度，有助于提高模型的计算效率和泛化能力。\n",
        "\n",
        "最大池化的主要优势在于它能够保留图像中最显著的特征，同时减少计算量。通过丢弃非最大值的信息，模型能够更加集中地关注对于任务而言最重要的特征。这使得网络对于位置变化更加鲁棒，因为最大值对于小的平移或变形具有不变性。"
      ],
      "metadata": {
        "id": "iHpq4xbBcOI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 最大池化：朴素前向传播\n",
        "在函数`max_pool_forward_naive`中实现最大池化操作的前向传播。同样，不必过于担心计算效率。"
      ],
      "metadata": {
        "id": "4-ttT219cbD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def max_pool_forward_naive(x, pool_param):\n",
        "    \"\"\"\n",
        "    A naive implementation of the forward pass for a max pooling layer.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C, H, W)\n",
        "    - pool_param: dictionary with the following keys:\n",
        "      - 'pool_height': The height of each pooling region\n",
        "      - 'pool_width': The width of each pooling region\n",
        "      - 'stride': The distance between adjacent pooling regions\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output data\n",
        "    - cache: (x, pool_param)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    #############################################################################\n",
        "    # TODO: Implement the max pooling forward pass                              #\n",
        "    #############################################################################\n",
        "    pool_height = pool_param['pool_height']\n",
        "    pool_width = pool_param['pool_width']\n",
        "    stride = pool_param['stride']\n",
        "    N, C, H, W = x.shape\n",
        "\n",
        "    H_out = 1 + (H - pool_height) // stride\n",
        "    W_out = 1 + (W - pool_width) // stride\n",
        "\n",
        "    out = np.zeros((N, C, H_out, W_out))\n",
        "    for n in range(N):\n",
        "      for c in range(C):\n",
        "        for i in range(H_out):\n",
        "          for j in range(W_out):\n",
        "            h_start = i*stride\n",
        "            h_end = h_start + pool_height\n",
        "            w_start = j*stride\n",
        "            w_end = w_start + pool_width\n",
        "            out[n,c,i,j] = np.max(x[n, c, h_start:h_end, w_start:w_end])\n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "    cache = (x, pool_param)\n",
        "    return out, cache"
      ],
      "metadata": {
        "id": "iJzV11X2ckF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "您可以通过运行以下测试您的实现，您应该看到误差小于 ``1e-7``。"
      ],
      "metadata": {
        "id": "fCBALMakcif3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_shape = (2, 3, 4, 4)\n",
        "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
        "pool_param = {'pool_width': 2, 'pool_height': 2, 'stride': 2}\n",
        "\n",
        "out, _ = max_pool_forward_naive(x, pool_param)\n",
        "\n",
        "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
        "                          [-0.20421053, -0.18947368]],\n",
        "                         [[-0.14526316, -0.13052632],\n",
        "                          [-0.08631579, -0.07157895]],\n",
        "                         [[-0.02736842, -0.01263158],\n",
        "                          [ 0.03157895,  0.04631579]]],\n",
        "                        [[[ 0.09052632,  0.10526316],\n",
        "                          [ 0.14947368,  0.16421053]],\n",
        "                         [[ 0.20842105,  0.22315789],\n",
        "                          [ 0.26736842,  0.28210526]],\n",
        "                         [[ 0.32631579,  0.34105263],\n",
        "                          [ 0.38526316,  0.4       ]]]])\n",
        "\n",
        "# Compare your output with ours. Difference should be around 1e-8.\n",
        "print ('Testing max_pool_forward_naive function:')\n",
        "print ('max pool forward error: ', rel_error(out, correct_out))"
      ],
      "metadata": {
        "id": "uo2Pqq43cftO",
        "outputId": "1f8c36ac-4518-45e5-b896-b74aeba73a39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing max_pool_forward_naive function:\n",
            "max pool forward error:  4.1666665157267834e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 最大池化：朴素反向传播\n",
        "在函数`max_pool_backward_naive`中实现最大池化操作的反向传播。不必担心计算效率。\n",
        "\n"
      ],
      "metadata": {
        "id": "SHxOKrcrcqNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def max_pool_backward_naive(dout, cache):\n",
        "    \"\"\"\n",
        "    A naive implementation of the backward pass for a max pooling layer.\n",
        "\n",
        "    Inputs:\n",
        "    - dout: Upstream derivatives\n",
        "    - cache: A tuple of (x, pool_param) as in the forward pass.\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx = None\n",
        "    #############################################################################\n",
        "    # TODO: Implement the max pooling backward pass                             #\n",
        "    #############################################################################\n",
        "    pool_height = pool_param['pool_height']\n",
        "    pool_width = pool_param['pool_width']\n",
        "    stride = pool_param['stride']\n",
        "    N, C, H, W = x.shape\n",
        "\n",
        "    H_out = 1 + (H - pool_height) // stride\n",
        "    W_out = 1 + (W - pool_width) // stride\n",
        "\n",
        "    dx = np.zeros_like(x)\n",
        "    max_indices = np.zeros((N, C, H_out, W_out))\n",
        "\n",
        "    out = np.zeros((N, C, H_out, W_out))\n",
        "    for n in range(N):\n",
        "      for c in range(C):\n",
        "        for i in range(H_out):\n",
        "          for j in range(W_out):\n",
        "            h_start = i*stride\n",
        "            h_end = h_start + pool_height\n",
        "            w_start = j*stride\n",
        "            w_end = w_start + pool_width\n",
        "\n",
        "            window = x[n, c, h_start:h_end, w_start:w_end]\n",
        "            out[n,c,i,j] = np.max(window)\n",
        "\n",
        "            max_index = np.unravel_index(np.argmax(window), window.shape)\n",
        "            h_max = h_start + max_index[0]\n",
        "            w_max = w_start + max_index[1]\n",
        "\n",
        "            dx[n, c, h_max, w_max] += dout[n, c, i, j]\n",
        "\n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "    return dx\n"
      ],
      "metadata": {
        "id": "FGSTv5wPcwd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "完成后，运行以下内容以使用数值梯度检查验证您的反向传播，您应该看到误差小于 ``1e-10``。"
      ],
      "metadata": {
        "id": "xy0aMGKpcw9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.randn(3, 2, 8, 8)\n",
        "dout = np.random.randn(3, 2, 4, 4)\n",
        "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: max_pool_forward_naive(x, pool_param)[0], x, dout)\n",
        "\n",
        "out, cache = max_pool_forward_naive(x, pool_param)\n",
        "dx = max_pool_backward_naive(dout, cache)\n",
        "\n",
        "# Your error should be around 1e-12\n",
        "print ('Testing max_pool_backward_naive function:')\n",
        "print ('dx error: ', rel_error(dx, dx_num))"
      ],
      "metadata": {
        "id": "pawSV8Vucxcr",
        "outputId": "497b510f-7278-444c-df9f-1ae1935246b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing max_pool_backward_naive function:\n",
            "dx error:  3.2756244991678825e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 批标准化（Batch Normalization）\n",
        "\n",
        "让深度网络更容易训练的一种方法是使用更复杂的优化程序，如SGD+动量、RMSProp或Adam。另一种策略是改变网络的架构，使其更容易训练。沿着这些思路的一个想法是批归一化，由[1]提出。\n",
        "\n",
        "这个想法相对直观。当机器学习方法的输入数据由均值为零且方差为单位的不相关特征组成时，它们往往表现更好。在训练神经网络时，我们可以在将数据馈送到网络之前对数据进行预处理，以显式地去相关其特征；这将确保网络的第一层看到符合良好分布的数据。然而，即使我们对输入数据进行预处理，网络更深层的激活可能不再是不相关的，也可能不再具有零均值或单位方差，因为它们是网络中较早层的输出。更糟糕的是，在训练过程中，网络每一层的特征分布会随着每一层的权重更新而发生变化。\n",
        "\n",
        "[1]的作者假设深度神经网络内部特征的分布变化可能会使训练深度网络变得更加困难。为了解决这个问题，[1]提出在网络中插入批归一化层。在训练时，批归一化层使用一个小批量的数据来估计每个特征的均值和标准差。然后，使用这些估计的均值和标准差来居中和归一化小批量的特征。在训练过程中，保持这些均值和标准差的运行平均值，而在测试时，使用这些运行平均值来居中和归一化特征。\n",
        "\n",
        "这种归一化策略可能会降低网络的表示能力，因为对于某些层来说，具有非零均值或非单位方差的特征有时可能更为优化。因此，批归一化层包含每个特征维度的可学习的位移和缩放参数。\n",
        "\n",
        "\n",
        "批标准化是训练深度全连接网络的一种非常有用的技术。批标准化也可以用于卷积网络，但我们需要稍作调整；这个修改被称为“空间批标准化”。\n",
        "\n",
        "通常，批标准化接受形状为`(N, D)`的输入，并产生形状为`(N, D)`的输出，其中我们对小批量维度`N`进行归一化。对于来自卷积层的数据，批标准化需要接受形状为`(N, C, H, W)`的输入，并产生形状为`(N, C, H, W)`的输出，其中`N`维度表示小批量大小，`(H, W)`维度表示特征图的空间大小。\n",
        "\n",
        "如果特征图是通过卷积产生的，那么我们期望每个特征通道的统计信息在不同图像之间以及同一图像中的不同位置之间是相对一致的。因此，空间批标准化通过在小批量维度`N`和空间维度`H`和`W`上计算统计信息，为每个`C`特征通道计算均值和方差。\n",
        "\n",
        "[1] Sergey Ioffe和Christian Szegedy，“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”，ICML 2015。\n"
      ],
      "metadata": {
        "id": "vznGMJfJbsPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 批标准化：前向传播\n",
        "\n",
        "在函数`batchnorm_forward`中实现空间批标准化的前向传播。"
      ],
      "metadata": {
        "id": "NX-pUqcSd9VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def batchnorm_forward(x, gamma, beta, bn_param):\n",
        "    \"\"\"\n",
        "    Forward pass for batch normalization.\n",
        "\n",
        "    During training the sample mean and (uncorrected) sample variance are\n",
        "    computed from minibatch statistics and used to normalize the incoming data.\n",
        "    During training we also keep an exponentially decaying running mean of the mean\n",
        "    and variance of each feature, and these averages are used to normalize data\n",
        "    at test-time.\n",
        "\n",
        "    At each timestep we update the running averages for mean and variance using\n",
        "    an exponential decay based on the momentum parameter:\n",
        "\n",
        "    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
        "    running_var = momentum * running_var + (1 - momentum) * sample_var\n",
        "\n",
        "    Note that the batch normalization paper suggests a different test-time\n",
        "    behavior: they compute sample mean and variance for each feature using a\n",
        "    large number of training images rather than using a running average. For\n",
        "    this implementation we have chosen to use running averages instead since\n",
        "    they do not require an additional estimation step; the torch7 implementation\n",
        "    of batch normalization also uses running averages.\n",
        "\n",
        "    Input:\n",
        "    - x: Data of shape (N, D)\n",
        "    - gamma: Scale parameter of shape (D,)\n",
        "    - beta: Shift paremeter of shape (D,)\n",
        "    - bn_param: Dictionary with the following keys:\n",
        "      - mode: 'train' or 'test'; required\n",
        "      - eps: Constant for numeric stability\n",
        "      - momentum: Constant for running mean / variance.\n",
        "      - running_mean: Array of shape (D,) giving running mean of features\n",
        "      - running_var Array of shape (D,) giving running variance of features\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: of shape (N, D)\n",
        "    - cache: A tuple of values needed in the backward pass\n",
        "    \"\"\"\n",
        "    mode = bn_param['mode']\n",
        "    eps = bn_param.get('eps', 1e-5)\n",
        "    momentum = bn_param.get('momentum', 0.9)\n",
        "\n",
        "    N, D = x.shape\n",
        "    running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
        "    running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))\n",
        "\n",
        "    out, cache = None, None\n",
        "    if mode == 'train':\n",
        "        #############################################################################\n",
        "        # TODO: Implement the training-time forward pass for batch normalization.   #\n",
        "        # Use minibatch statistics to compute the mean and variance, use these      #\n",
        "        # statistics to normalize the incoming data, and scale and shift the        #\n",
        "        # normalized data using gamma and beta.                                     #\n",
        "        #                                                                           #\n",
        "        # You should store the output in the variable out. Any intermediates that   #\n",
        "        # you need for the backward pass should be stored in the cache variable.    #\n",
        "        #                                                                           #\n",
        "        # You should also use your computed sample mean and variance together with  #\n",
        "        # the momentum variable to update the running mean and running variance,    #\n",
        "        # storing your result in the running_mean and running_var variables.        #\n",
        "        #############################################################################\n",
        "\n",
        "        sample_mean = x.mean(axis=(0))\n",
        "        sample_var = x.var(axis=(0))\n",
        "\n",
        "        x_normalized = (x - sample_mean)/np.sqrt(sample_var)\n",
        "        out = gamma*x_normalized + beta\n",
        "\n",
        "        running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
        "        running_var = momentum * running_var + (1 - momentum) * sample_var\n",
        "\n",
        "        cache = (x, x_normalized, sample_mean, sample_var, gamma, beta, eps)\n",
        "        #############################################################################\n",
        "        #                             END OF YOUR CODE                              #\n",
        "        #############################################################################\n",
        "    elif mode == 'test':\n",
        "        #############################################################################\n",
        "        # TODO: Implement the test-time forward pass for batch normalization. Use   #\n",
        "        # the running mean and variance to normalize the incoming data, then scale  #\n",
        "        # and shift the normalized data using gamma and beta. Store the result in   #\n",
        "        # the out variable.                                                         #\n",
        "        #############################################################################\n",
        "        sample_mean = x.mean(axis=(0))\n",
        "        sample_var = x.var(axis=(0))\n",
        "\n",
        "        running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
        "        running_var = momentum * running_var + (1 - momentum) * sample_var\n",
        "\n",
        "        x_normalized = (x - running_mean)/np.sqrt(running_var)\n",
        "        out = gamma*x_normalized + beta\n",
        "        #############################################################################\n",
        "        #                             END OF YOUR CODE                              #\n",
        "        #############################################################################\n",
        "    else:\n",
        "        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
        "\n",
        "    # Store the updated running means back into bn_param\n",
        "    bn_param['running_mean'] = running_mean\n",
        "    bn_param['running_var'] = running_var\n",
        "\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def spatial_batchnorm_forward(x, gamma, beta, bn_param):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for spatial batch normalization.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data of shape (N, C, H, W)\n",
        "    - gamma: Scale parameter, of shape (C,)\n",
        "    - beta: Shift parameter, of shape (C,)\n",
        "    - bn_param: Dictionary with the following keys:\n",
        "      - mode: 'train' or 'test'; required\n",
        "      - eps: Constant for numeric stability\n",
        "      - momentum: Constant for running mean / variance. momentum=0 means that\n",
        "        old information is discarded completely at every time step, while\n",
        "        momentum=1 means that new information is never incorporated. The\n",
        "        default of momentum=0.9 should work well in most situations.\n",
        "      - running_mean: Array of shape (D,) giving running mean of features\n",
        "      - running_var Array of shape (D,) giving running variance of features\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output data, of shape (N, C, H, W)\n",
        "    - cache: Values needed for the backward pass\n",
        "    \"\"\"\n",
        "    N, C, H, W = x.shape\n",
        "    x_flat = x.transpose(0, 2, 3, 1).reshape(-1, C)\n",
        "    out_flat, cache = batchnorm_forward(x_flat, gamma, beta, bn_param)\n",
        "    out = out_flat.reshape(N, H, W, C).transpose(0, 3, 1, 2)\n",
        "    return out, cache"
      ],
      "metadata": {
        "id": "9AcxZtddeDuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "通过运行以下内容检查您的实现，您应该看到误差小于 ``1e-5``。"
      ],
      "metadata": {
        "id": "LrAcJtY1eDEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the training-time forward pass by checking means and variances\n",
        "# of features both before and after spatial batch normalization\n",
        "import numpy as np\n",
        "\n",
        "N, C, H, W = 2, 3, 4, 5\n",
        "x = 4 * np.random.randn(N, C, H, W) + 10\n",
        "\n",
        "print ('Before spatial batch normalization:')\n",
        "print ('  Shape: ', x.shape)\n",
        "print ('  Means: ', x.mean(axis=(0, 2, 3)))\n",
        "print ('  Stds: ', x.std(axis=(0, 2, 3)))\n",
        "\n",
        "# Means should be close to zero and stds close to one. Shape should be unchanged.\n",
        "gamma, beta = np.ones(C), np.zeros(C)\n",
        "bn_param = {'mode': 'train'}\n",
        "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
        "print ('After spatial batch normalization:')\n",
        "print ('  Shape: ', out.shape)\n",
        "print ('  Means: ', out.mean(axis=(0, 2, 3)))\n",
        "print ('  Stds: ', out.std(axis=(0, 2, 3)))\n",
        "print ('  Means error: ', out.mean(axis=(0, 2, 3)).mean())\n",
        "print ('  Stds error: ', (1 - out.std(axis=(0, 2, 3))).mean())\n",
        "\n",
        "\n",
        "# Means should be close to beta and stds close to gamma. Shape should be unchnaged.\n",
        "gamma, beta = np.asarray([3, 4, 5]), np.asarray([6, 7, 8])\n",
        "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
        "print ('After spatial batch normalization (nontrivial gamma, beta):')\n",
        "print ('  Shape: ', out.shape)\n",
        "print ('  Means: ', out.mean(axis=(0, 2, 3)))\n",
        "print ('  Stds: ', out.std(axis=(0, 2, 3)))\n",
        "print ('  Means error: ', (beta - out.mean(axis=(0, 2, 3))).mean())\n",
        "print ('  Stds error: ', (gamma - out.std(axis=(0, 2, 3))).mean())"
      ],
      "metadata": {
        "id": "Ug0ZZgJTeE1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c266ea13-9355-461f-fa61-73489dc544b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before spatial batch normalization:\n",
            "  Shape:  (2, 3, 4, 5)\n",
            "  Means:  [9.70660725 8.94972907 9.43287164]\n",
            "  Stds:  [3.56684828 3.91649695 3.50278471]\n",
            "After spatial batch normalization:\n",
            "  Shape:  (2, 3, 4, 5)\n",
            "  Means:  [-2.67841305e-16 -8.32667268e-17  3.16413562e-16]\n",
            "  Stds:  [1. 1. 1.]\n",
            "  Means error:  -1.1564823173178714e-17\n",
            "  Stds error:  -7.401486830834377e-17\n",
            "After spatial batch normalization (nontrivial gamma, beta):\n",
            "  Shape:  (2, 3, 4, 5)\n",
            "  Means:  [6. 7. 8.]\n",
            "  Stds:  [3. 4. 5.]\n",
            "  Means error:  2.960594732333751e-15\n",
            "  Stds error:  -4.440892098500626e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 批标准化：反向传播\n",
        "\n",
        "在函数`spatial_batchnorm_backward`中实现空间批标准化的反向传播。"
      ],
      "metadata": {
        "id": "XB5L5fqYeQDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batchnorm_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Backward pass for batch normalization.\n",
        "\n",
        "    For this implementation, you should write out a computation graph for\n",
        "    batch normalization on paper and propagate gradients backward through\n",
        "    intermediate nodes.\n",
        "\n",
        "    Inputs:\n",
        "    - dout: Upstream derivatives, of shape (N, D)\n",
        "    - cache: Variable of intermediates from batchnorm_forward.\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to inputs x, of shape (N, D)\n",
        "    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n",
        "    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n",
        "    \"\"\"\n",
        "    dx, dgamma, dbeta = None, None, None\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO: Implement the backward pass for batch normalization. Store the      #\n",
        "    # results in the dx, dgamma, and dbeta variables.                           #\n",
        "    #############################################################################\n",
        "    x, x_normalized, sample_mean, sample_var, gamma, beta, eps = cache\n",
        "    N, D = dout.shape\n",
        "    dbeta = np.sum(dout, axis=0)\n",
        "    dgamma = np.sum(dout * x_normalized, axis=0)\n",
        "    dx_normalized = dout * gamma\n",
        "    dvar = np.sum(dx_normalized * (x - sample_mean) * (-0.5) * (sample_var + eps) ** (-1.5), axis=0)\n",
        "    dmean = np.sum(dx_normalized * (-1 / np.sqrt(sample_var + eps)), axis=0) + \\\n",
        "            dvar * np.sum(-2 * (x - sample_mean), axis=0) / N\n",
        "\n",
        "    dx = dx_normalized / np.sqrt(sample_var + eps) + \\\n",
        "         dvar * 2 * (x - sample_mean) / N + \\\n",
        "         dmean / N\n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "\n",
        "    return dx, dgamma, dbeta\n",
        "\n",
        "\n",
        "def spatial_batchnorm_backward(dout, cache):\n",
        "  \"\"\"\n",
        "  Computes the backward pass for spatial batch normalization.\n",
        "\n",
        "  Inputs:\n",
        "  - dout: Upstream derivatives, of shape (N, C, H, W)\n",
        "  - cache: Values from the forward pass\n",
        "\n",
        "  Returns a tuple of:\n",
        "  - dx: Gradient with respect to inputs, of shape (N, C, H, W)\n",
        "  - dgamma: Gradient with respect to scale parameter, of shape (C,)\n",
        "  - dbeta: Gradient with respect to shift parameter, of shape (C,)\n",
        "  \"\"\"\n",
        "  N, C, H, W = dout.shape\n",
        "  dout_flat = dout.transpose(0, 2, 3, 1).reshape(-1, C)\n",
        "  dx_flat, dgamma, dbeta = batchnorm_backward(dout_flat, cache)\n",
        "  dx = dx_flat.reshape(N, H, W, C).transpose(0, 3, 1, 2)\n",
        "  return dx, dgamma, dbeta"
      ],
      "metadata": {
        "id": "hSk2XU8ZeWbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "通过运行以下内容使用数值梯度检查来检查您的实现，您应该看到误差小于 1e-7。"
      ],
      "metadata": {
        "id": "i9PI47XmeWsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, C, H, W = 2, 3, 4, 5\n",
        "x = 5 * np.random.randn(N, C, H, W) + 12\n",
        "gamma = np.random.randn(C)\n",
        "beta = np.random.randn(C)\n",
        "dout = np.random.randn(N, C, H, W)\n",
        "\n",
        "bn_param = {'mode': 'train'}\n",
        "fx = lambda x: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
        "fg = lambda a: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
        "fb = lambda b: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
        "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
        "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
        "\n",
        "_, cache = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
        "dx, dgamma, dbeta = spatial_batchnorm_backward(dout, cache)\n",
        "print ('dx error: ', rel_error(dx_num, dx))\n",
        "print ('dgamma error: ', rel_error(da_num, dgamma))\n",
        "print ('dbeta error: ', rel_error(db_num, dbeta))"
      ],
      "metadata": {
        "id": "uIHGYK4MebDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7d71b9-8c09-4c66-f973-cac56cbf6c7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dx error:  5.397506808940468e-07\n",
            "dgamma error:  1.719315643026913e-11\n",
            "dbeta error:  4.333467383368165e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 结语\n",
        "\n",
        "恭喜你！你已经完成了第一次作业。尽管这一路历经艰辛，但是你对于卷积、池化、批标准化都有了更加深刻的理解！\n",
        "\n",
        "\n",
        "\n",
        ">本次作业负责人：郜今（助教），gaojin@sjtu.edu.cn。\n",
        "最后请允许我再次强调，作业在 Canvas 上提交，只需要上传一份 ipynb 文件，请保留每个单元格的运行结果，注意时间节点。 如有任何问题，请联系[助教](https://cs7353.netlify.app/staff/)。"
      ],
      "metadata": {
        "id": "dWYhbqCweiHj"
      }
    }
  ]
}