{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "欢迎来到上海交通大学 CS7353（2025年春季学期）《[设计和理解深度神经网络](https://cs7353.netlify.app/)》！\n",
        "\n",
        "这里是第二次课程作业，具体时间信息见[课程网站](https://cs7353.netlify.app/)。作业在 Canvas 上提交，注意时间节点。只需要上传一份 ipynb 文件，请务必保留每个单元格的运行结果。\n",
        "\n",
        "如有任何问题，请联系[助教](https://cs7353.netlify.app/staff/)。\n",
        "\n",
        "# 1 简介\n",
        "\n",
        "在本次作业中，您将练习编写 recurrent neural networks (RNN) 代码。本次作业不需要用到GPU。\n",
        "\n",
        "此任务的目标如下：\n",
        "\n",
        "- 了解循环神经网络（RNNs）的架构\n",
        "- 了解词嵌入（Word Embedding）\n",
        "- 了解时序仿射层和损失\n",
        "\n",
        "注意，请严格遵守以下注意事项，如有违背，本次作业零分处理：\n",
        "- 请仅在标明的 TODO 位置完成代码，请勿更改其他代码；\n",
        "- 请勿 import 其他 python package（补充：请手搓，不要为了省事直接调用打包好的函数）；\n",
        "- 请务必保留每个单元格的运行结果。"
      ],
      "metadata": {
        "id": "nP73M5d0QiXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 准备：辅助函数\n",
        "\n",
        "这里是用于检测结果的辅助函数，**您不必进行任何代码层面的操作**，仅需运行对应单元格。"
      ],
      "metadata": {
        "id": "-iLUHDIfabrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
        "\n",
        "\n",
        "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
        "    \"\"\"\n",
        "    Evaluate a numeric gradient for a function that accepts a numpy\n",
        "    array and returns a numpy array.\n",
        "    \"\"\"\n",
        "    grad = np.zeros_like(x)\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h\n",
        "        pos = f(x).copy()\n",
        "        x[ix] = oldval - h\n",
        "        neg = f(x).copy()\n",
        "        x[ix] = oldval\n",
        "\n",
        "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
        "        it.iternext()\n",
        "    return grad\n",
        "\n",
        "\n",
        "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
        "    \"\"\"\n",
        "    a naive implementation of numerical gradient of f at x\n",
        "    - f should be a function that takes a single argument\n",
        "    - x is the point (numpy array) to evaluate the gradient at\n",
        "    \"\"\"\n",
        "\n",
        "    fx = f(x) # evaluate function value at original point\n",
        "    grad = np.zeros_like(x)\n",
        "    # iterate over all indexes in x\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "\n",
        "        # evaluate function at x+h\n",
        "        ix = it.multi_index\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h # increment by h\n",
        "        fxph = f(x) # evalute f(x + h)\n",
        "        x[ix] = oldval - h\n",
        "        fxmh = f(x) # evaluate f(x - h)\n",
        "        x[ix] = oldval # restore\n",
        "\n",
        "        # compute the partial derivative with centered formula\n",
        "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
        "        if verbose:\n",
        "            print(ix, grad[ix])\n",
        "        it.iternext() # step to next dimension\n",
        "\n",
        "    return grad"
      ],
      "metadata": {
        "id": "7BpuEnwWNeMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52a1ac27-8912-40b0-b199-6a12c06b15c5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 循环神经网络—单步（RNN-step）\n",
        "循环神经网络（Recurrent Neural Network，简称RNN）是一类专门用于处理序列数据的神经网络架构。与传统的前馈神经网络不同，RNN 具有循环连接，使其能够在处理序列数据时保留先前的信息状态。这种结构使得 RNN 在语言模型、时间序列分析、语音识别等任务上表现出色。\n",
        "\n",
        "RNN 的主要特点在于它引入了一个循环单元，允许网络在处理序列数据时记住先前的信息。这个循环单元可以被看作是网络内部的一个记忆单元，能够保存之前的状态，并将这个状态传递到下一个时间步。这样，网络就能够捕捉到数据中的时间依赖关系，对于时序数据的建模更加灵活。\n",
        "\n",
        "在每个时间步，RNN 接收当前的输入和上一个时间步的隐藏状态，然后计算新的隐藏状态。这种递归结构使得网络能够考虑到先前的信息，从而更好地适应序列数据的模式。"
      ],
      "metadata": {
        "id": "o9z-PRJNcG__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 循环神经网络（单步）：前向传播\n",
        "\n",
        "首先实现函数 ``rnn_step_forward``，该函数实现基本循环神经网络单个时间步的前向传播。"
      ],
      "metadata": {
        "id": "zMYPMMAxY-2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_step_forward(x, prev_h, Wx, Wh, b):\n",
        "    \"\"\"\n",
        "    Run the forward pass for a single timestep of a vanilla RNN that uses a tanh\n",
        "    activation function.\n",
        "\n",
        "    The input data has dimension D, the hidden state has dimension H, and we use\n",
        "    a minibatch size of N.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data for this timestep, of shape (N, D).\n",
        "    - prev_h: Hidden state from previous timestep, of shape (N, H)\n",
        "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
        "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
        "    - b: Biases of shape (H,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - next_h: Next hidden state, of shape (N, H)\n",
        "    - cache: Tuple of values needed for the backward pass.\n",
        "    \"\"\"\n",
        "    next_h, cache = None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement a single forward step for the vanilla RNN. Store the next  #\n",
        "    # hidden state and any values you need for the backward pass in the next_h   #\n",
        "    # and cache variables respectively.                                          #\n",
        "    ##############################################################################\n",
        "    a = prev_h @ Wh + x @ Wx + b\n",
        "    next_h = np.tanh(a)\n",
        "    cache = (x, prev_h, Wx, Wh, a)\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return next_h, cache"
      ],
      "metadata": {
        "id": "EpoTnPSTZWyA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "完成后，运行以下代码以检查您的实现。您应该看到误差小于 ``1e-8``。"
      ],
      "metadata": {
        "id": "utGLyEEZZZlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, D, H = 3, 10, 4\n",
        "\n",
        "x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n",
        "prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n",
        "Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n",
        "Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n",
        "b = np.linspace(-0.2, 0.4, num=H)\n",
        "\n",
        "next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
        "expected_next_h = np.asarray([\n",
        "  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
        "  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n",
        "  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n",
        "\n",
        "print('next_h error: ', rel_error(expected_next_h, next_h))"
      ],
      "metadata": {
        "id": "U0iaZH30Yp3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e9b27c-282d-44c7-ee4b-3d4210b84fd8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "next_h error:  6.292421426471037e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 循环神经网络（单步）：反向传播\n",
        "\n",
        "实现 ``rnn_step_backward`` 函数。"
      ],
      "metadata": {
        "id": "uw5sC0F8Z9nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def rnn_step_backward(dnext_h, cache):\n",
        "    \"\"\"\n",
        "    Backward pass for a single timestep of a vanilla RNN.\n",
        "\n",
        "    Inputs:\n",
        "    - dnext_h: Gradient of loss with respect to next hidden state\n",
        "    - cache: Cache object from the forward pass\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradients of input data, of shape (N, D)\n",
        "    - dprev_h: Gradients of previous hidden state, of shape (N, H)\n",
        "    - dWx: Gradients of input-to-hidden weights, of shape (D, H)\n",
        "    - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)\n",
        "    - db: Gradients of bias vector, of shape (H,)\n",
        "    \"\"\"\n",
        "    dx, dprev_h, dWx, dWh, db = None, None, None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the backward pass for a single step of a vanilla RNN.      #\n",
        "    #                                                                            #\n",
        "    # HINT: For the tanh function, you can compute the local derivative in terms #\n",
        "    # of the output value from tanh.                                             #\n",
        "    ##############################################################################\n",
        "    x, prev_h, Wx, Wh, a = cache\n",
        "    da = dnext_h * (1- np.tanh(a)**2)\n",
        "    dx = da @ Wx.T\n",
        "    dprev_h = da @ Wh.T\n",
        "    dWx = x.T @ da\n",
        "    dWh = prev_h.T @ da\n",
        "    db = np.sum(da, axis = 0)\n",
        "\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return dx, dprev_h, dWx, dWh, db\n"
      ],
      "metadata": {
        "id": "vkuLPj0paBn1"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "完成后，运行以下代码进行数值梯度检查。您应该看到误差小于 ``1e-8``。"
      ],
      "metadata": {
        "id": "NJTPMxxaaB_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(231)\n",
        "N, D, H = 4, 5, 6\n",
        "x = np.random.randn(N, D)\n",
        "h = np.random.randn(N, H)\n",
        "Wx = np.random.randn(D, H)\n",
        "Wh = np.random.randn(H, H)\n",
        "b = np.random.randn(H)\n",
        "\n",
        "out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n",
        "\n",
        "dnext_h = np.random.randn(*out.shape)\n",
        "\n",
        "fx = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fh = lambda prev_h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fb = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n",
        "dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n",
        "dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n",
        "dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n",
        "db_num = eval_numerical_gradient_array(fb, b, dnext_h)\n",
        "\n",
        "dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n",
        "\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n",
        "print('dWx error: ', rel_error(dWx_num, dWx))\n",
        "print('dWh error: ', rel_error(dWh_num, dWh))\n",
        "print('db error: ', rel_error(db_num, db))"
      ],
      "metadata": {
        "id": "pDpcA42MaEHN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cf1f6bc-4682-4449-d276-87dd5d1a5dd1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dx error:  2.7795541640745535e-10\n",
            "dprev_h error:  2.732467428030486e-10\n",
            "dWx error:  9.709219069305414e-10\n",
            "dWh error:  5.034262638717296e-10\n",
            "db error:  1.708752322503098e-11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 循环神经网络（RNN）\n",
        "\n",
        "\n",
        "现在您已经实现了基本 RNN 单个时间步的前向和反向传播，您将组合这些部分来实现处理整个数据序列的RNN。"
      ],
      "metadata": {
        "id": "iHpq4xbBcOI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 循环神经网络：前向传播\n",
        "\n",
        "实现函数 ``rnn_forward``。这应该使用您上面定义的 ``rnn_step_forward`` 函数来实现。"
      ],
      "metadata": {
        "id": "4-ttT219cbD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def rnn_forward(x, h0, Wx, Wh, b):\n",
        "    \"\"\"\n",
        "    Run a vanilla RNN forward on an entire sequence of data. We assume an input\n",
        "    sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n",
        "    size of H, and we work over a minibatch containing N sequences. After running\n",
        "    the RNN forward, we return the hidden states for all timesteps.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data for the entire timeseries, of shape (N, T, D).\n",
        "    - h0: Initial hidden state, of shape (N, H)\n",
        "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
        "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
        "    - b: Biases of shape (H,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - h: Hidden states for the entire timeseries, of shape (N, T, H).\n",
        "    - cache: Values needed in the backward pass\n",
        "    \"\"\"\n",
        "    h, cache = None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass for a vanilla RNN running on a sequence of    #\n",
        "    # input data. You should use the rnn_step_forward function that you defined  #\n",
        "    # above. You can use a for loop to help compute the forward pass.            #\n",
        "    ##############################################################################\n",
        "\n",
        "    T = x.shape[1]\n",
        "    h_list = []\n",
        "    cache = []\n",
        "    for t in range(T):\n",
        "        if t == 0:\n",
        "          h_prev = h0\n",
        "        x_t = x[:, t, :]\n",
        "        h_next, cache_t = rnn_step_forward(x_t, h_prev, Wx, Wh, b)\n",
        "        h_list.append(h_next)\n",
        "        cache.append(cache_t)\n",
        "        h_prev = h_next\n",
        "    pass\n",
        "\n",
        "    h = np.stack(h_list,axis = 1)\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return h, cache\n"
      ],
      "metadata": {
        "id": "iJzV11X2ckF7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "完成后，运行以下代码检查您的实现。您应该看到误差小于 ``1e-7``。"
      ],
      "metadata": {
        "id": "fCBALMakcif3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, T, D, H = 2, 3, 4, 5\n",
        "\n",
        "x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n",
        "h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n",
        "Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n",
        "Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n",
        "b = np.linspace(-0.7, 0.1, num=H)\n",
        "\n",
        "h, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
        "expected_h = np.asarray([\n",
        "  [\n",
        "    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n",
        "    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n",
        "    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n",
        "  ],\n",
        "  [\n",
        "    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n",
        "    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n",
        "    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n",
        "print('h error: ', rel_error(expected_h, h))"
      ],
      "metadata": {
        "id": "uo2Pqq43cftO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c55c0ec-5e2d-4c8b-f96c-554ab1a217bd"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h error:  7.728466151011529e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 循环神经网络：反向传播\n",
        "\n",
        "在函数`rnn_backward`中实现循环神经网络的反向传播。不必担心计算效率。\n",
        "\n"
      ],
      "metadata": {
        "id": "SHxOKrcrcqNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_backward(dh, cache):\n",
        "    \"\"\"\n",
        "    Compute the backward pass for a vanilla RNN over an entire sequence of data.\n",
        "\n",
        "    Inputs:\n",
        "    - dh: Upstream gradients of all hidden states, of shape (N, T, H)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient of inputs, of shape (N, T, D)\n",
        "    - dh0: Gradient of initial hidden state, of shape (N, H)\n",
        "    - dWx: Gradient of input-to-hidden weights, of shape (D, H)\n",
        "    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)\n",
        "    - db: Gradient of biases, of shape (H,)\n",
        "    \"\"\"\n",
        "    dx, dh0, dWx, dWh, db = None, None, None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the backward pass for a vanilla RNN running an entire      #\n",
        "    # sequence of data. You should use the rnn_step_backward function that you   #\n",
        "    # defined above. You can use a for loop to help compute the backward pass.   #\n",
        "    ##############################################################################\n",
        "    N, T, H = dh.shape\n",
        "    x0, _ , Wx, Wh, _ = cache[0]\n",
        "    D = x0.shape[1]\n",
        "\n",
        "    dx = np.zeros((N, T, D))\n",
        "    dWx = np.zeros((D, H))\n",
        "    dWh = np.zeros((H, H))\n",
        "    db = np.zeros(H)\n",
        "\n",
        "    dprev_h_t = np.zeros((N, H)) #gradient from next hidden state\n",
        "\n",
        "    for t in reversed(range(T)):\n",
        "        dh_t = dh[:,t,:] + dprev_h_t\n",
        "        dx_t, dprev_h_t, dWx_t, dWh_t, db_t = rnn_step_backward(dh_t, cache[t])\n",
        "        dx[:, t, :] += dx_t\n",
        "        dWx += dWx_t\n",
        "        dWh += dWh_t\n",
        "        db +=db_t\n",
        "\n",
        "    dh0 = dprev_h_t\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return dx, dh0, dWx, dWh, db"
      ],
      "metadata": {
        "id": "FGSTv5wPcwd4"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "通过运行以下内容使用数值梯度检查来检查您的实现。您应该看到误差小于 ``1e-6``。"
      ],
      "metadata": {
        "id": "xy0aMGKpcw9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(231)\n",
        "\n",
        "N, D, T, H = 2, 3, 10, 5\n",
        "\n",
        "x = np.random.randn(N, T, D)\n",
        "h0 = np.random.randn(N, H)\n",
        "Wx = np.random.randn(D, H)\n",
        "Wh = np.random.randn(H, H)\n",
        "b = np.random.randn(H)\n",
        "\n",
        "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
        "\n",
        "dout = np.random.randn(*out.shape)\n",
        "\n",
        "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
        "\n",
        "fx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
        "dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n",
        "dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n",
        "dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n",
        "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
        "\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dh0 error: ', rel_error(dh0_num, dh0))\n",
        "print('dWx error: ', rel_error(dWx_num, dWx))\n",
        "print('dWh error: ', rel_error(dWh_num, dWh))\n",
        "print('db error: ', rel_error(db_num, db))"
      ],
      "metadata": {
        "id": "pawSV8Vucxcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4e418ab-51a2-4a83-f05b-1d1401b9cb8f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dx error:  1.5354482248401769e-09\n",
            "dh0 error:  3.3830821485562176e-09\n",
            "dWx error:  7.23583883274483e-09\n",
            "dWh error:  1.3049601378601992e-07\n",
            "db error:  1.5197668388626435e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 词嵌入（Word Embedding）\n",
        "\n",
        "在深度学习系统中，我们通常使用向量表示单词。词汇表中的每个单词都将与一个向量关联，并且这些向量将与系统的其余部分一起学习。"
      ],
      "metadata": {
        "id": "vznGMJfJbsPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 词嵌入：前向传播\n",
        "\n",
        "实现函数 `word_embedding_forward`，将单词（用整数表示）转换为向量。"
      ],
      "metadata": {
        "id": "NX-pUqcSd9VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_embedding_forward(x, W):\n",
        "    \"\"\"\n",
        "    Forward pass for word embeddings. We operate on minibatches of size N where\n",
        "    each sequence has length T. We assume a vocabulary of V words, assigning each\n",
        "    to a vector of dimension D.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Integer array of shape (N, T) giving indices of words. Each element idx\n",
        "      of x muxt be in the range 0 <= idx < V.\n",
        "    - W: Weight matrix of shape (V, D) giving word vectors for all words.\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Array of shape (N, T, D) giving word vectors for all input words.\n",
        "    - cache: Values needed for the backward pass\n",
        "    \"\"\"\n",
        "    out, cache = None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the backward pass for word embeddings.                     #\n",
        "    #                                                                            #\n",
        "    # HINT: Look up the function np.add.at                                       #\n",
        "    ##############################################################################\n",
        "    N, T = x.shape\n",
        "    _, D = W.shape\n",
        "\n",
        "    out = np.zeros((N, T, D))\n",
        "    for n in range(N):\n",
        "      for t in range(T):\n",
        "         out[n,t,:]= W[x[n,t]]\n",
        "\n",
        "    cache = (x,W)\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return out, cache"
      ],
      "metadata": {
        "id": "9AcxZtddeDuP"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "运行以下代码检查您的实现。您应该看到误差小于 `1e-7`。"
      ],
      "metadata": {
        "id": "LrAcJtY1eDEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, T, V, D = 2, 4, 5, 3\n",
        "\n",
        "x = np.asarray([[0, 3, 1, 2], [2, 1, 0, 3]])\n",
        "W = np.linspace(0, 1, num=V*D).reshape(V, D)\n",
        "\n",
        "out, _ = word_embedding_forward(x, W)\n",
        "expected_out = np.asarray([\n",
        " [[ 0.,          0.07142857,  0.14285714],\n",
        "  [ 0.64285714,  0.71428571,  0.78571429],\n",
        "  [ 0.21428571,  0.28571429,  0.35714286],\n",
        "  [ 0.42857143,  0.5,         0.57142857]],\n",
        " [[ 0.42857143,  0.5,         0.57142857],\n",
        "  [ 0.21428571,  0.28571429,  0.35714286],\n",
        "  [ 0.,          0.07142857,  0.14285714],\n",
        "  [ 0.64285714,  0.71428571,  0.78571429]]])\n",
        "\n",
        "print('out error: ', rel_error(expected_out, out))"
      ],
      "metadata": {
        "id": "Ug0ZZgJTeE1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45e4d4da-249c-4959-e987-1f121ef4dafd"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out error:  1.0000000094736443e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 词嵌入：反向传播\n",
        "\n",
        "在函数 `word_embedding_backward` 中实现词嵌入函数的反向传播。"
      ],
      "metadata": {
        "id": "XB5L5fqYeQDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_embedding_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Backward pass for word embeddings. We cannot back-propagate into the words\n",
        "    since they are integers, so we only return gradient for the word embedding\n",
        "    matrix.\n",
        "\n",
        "    HINT: Look up the function np.add.at\n",
        "\n",
        "    Inputs:\n",
        "    - dout: Upstream gradients of shape (N, T, D)\n",
        "    - cache: Values from the forward pass\n",
        "\n",
        "    Returns:\n",
        "    - dW: Gradient of word embedding matrix, of shape (V, D).\n",
        "    \"\"\"\n",
        "    dW = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the backward pass for word embeddings.                     #\n",
        "    #                                                                            #\n",
        "    # Note that Words can appear more than once in a sequence.                   #\n",
        "    # HINT: Look up the function np.add.at                                       #\n",
        "    ##############################################################################\n",
        "    x , W = cache\n",
        "    V , _ = W.shape\n",
        "    dW = np.zeros_like(W)\n",
        "    np.add.at(dW, x, dout)\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return dW"
      ],
      "metadata": {
        "id": "hSk2XU8ZeWbr"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "完成后，运行以下代码进行数值梯度检查。您应该看到误差小于 `1e-11`。"
      ],
      "metadata": {
        "id": "i9PI47XmeWsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(231)\n",
        "\n",
        "N, T, V, D = 50, 3, 5, 6\n",
        "x = np.random.randint(V, size=(N, T))\n",
        "W = np.random.randn(V, D)\n",
        "\n",
        "out, cache = word_embedding_forward(x, W)\n",
        "dout = np.random.randn(*out.shape)\n",
        "dW = word_embedding_backward(dout, cache)\n",
        "\n",
        "f = lambda W: word_embedding_forward(x, W)[0]\n",
        "dW_num = eval_numerical_gradient_array(f, W, dout)\n",
        "\n",
        "print('dW error: ', rel_error(dW, dW_num))"
      ],
      "metadata": {
        "id": "uIHGYK4MebDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8feb909d-1053-40d0-fa31-80016f408f6e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dW error:  3.2774595693100364e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 时序仿射层和损失"
      ],
      "metadata": {
        "id": "ymYJST8-F6SB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 时序仿射层（Temporal Affine Layer）\n",
        "在每个时间步，我们使用仿射函数将该时间步的 RNN 隐藏向量转换为词汇表中每个单词的分数。\n",
        "在函数 `temporal_affine_forward` 中实现时间仿射层的前向传播，在函数 `temporal_affine_backward` 中实现时间仿射层的反向传播。"
      ],
      "metadata": {
        "id": "Q1hVISwlHD6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def temporal_affine_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Forward pass for a temporal affine layer. The input is a set of D-dimensional\n",
        "    vectors arranged into a minibatch of N timeseries, each of length T. We use\n",
        "    an affine function to transform each of those vectors into a new vector of\n",
        "    dimension M.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data of shape (N, T, D)\n",
        "    - w: Weights of shape (D, M)\n",
        "    - b: Biases of shape (M,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output data of shape (N, T, M)\n",
        "    - cache: Values needed for the backward pass\n",
        "    \"\"\"\n",
        "    out, cache = None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass for temporal affine.                      #\n",
        "    ##############################################################################\n",
        "    out = x @ w + b\n",
        "\n",
        "    cache = (x, w)\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def temporal_affine_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Backward pass for temporal affine layer.\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream gradients of shape (N, T, M)\n",
        "    - cache: Values from forward pass\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient of input, of shape (N, T, D)\n",
        "    - dw: Gradient of weights, of shape (D, M)\n",
        "    - db: Gradient of biases, of shape (M,)\n",
        "    \"\"\"\n",
        "    dx, dw, db = None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the backward pass for temporal affine.                     #\n",
        "    ##############################################################################\n",
        "    x, w = cache\n",
        "    N, T, D = x.shape\n",
        "    _, _, M = dout.shape\n",
        "    x_reshape = x.reshape(N*T, D)\n",
        "    dout_reshape = dout.reshape(N*T, M)\n",
        "\n",
        "    dx_squeeze = dout_reshape @ w.T\n",
        "    dw = x_reshape.T @ dout_reshape\n",
        "    db = np.sum(dout_reshape,axis=0)\n",
        "\n",
        "    dx = dx_squeeze.reshape(N,T,D)\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "    return dx, dw, db"
      ],
      "metadata": {
        "id": "qxEFuE-cGVfs"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "运行以下代码对实现进行数值梯度检查。您应该看到误差小于 `1e-9`。"
      ],
      "metadata": {
        "id": "9Y4_IEiRGUB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(231)\n",
        "\n",
        "# Gradient check for temporal affine layer\n",
        "N, T, D, M = 2, 3, 4, 5\n",
        "x = np.random.randn(N, T, D)\n",
        "w = np.random.randn(D, M)\n",
        "b = np.random.randn(M)\n",
        "\n",
        "out, cache = temporal_affine_forward(x, w, b)\n",
        "\n",
        "dout = np.random.randn(*out.shape)\n",
        "\n",
        "fx = lambda x: temporal_affine_forward(x, w, b)[0]\n",
        "fw = lambda w: temporal_affine_forward(x, w, b)[0]\n",
        "fb = lambda b: temporal_affine_forward(x, w, b)[0]\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
        "dw_num = eval_numerical_gradient_array(fw, w, dout)\n",
        "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
        "\n",
        "dx, dw, db = temporal_affine_backward(dout, cache)\n",
        "\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dw error: ', rel_error(dw_num, dw))\n",
        "print('db error: ', rel_error(db_num, db))"
      ],
      "metadata": {
        "id": "ClOSbNlCGYwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8f2257c-35f1-4f0b-ca59-f239a293ca84"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dx error:  2.9215945034030545e-10\n",
            "dw error:  1.5772088618663602e-10\n",
            "db error:  3.252200556967514e-11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 时序Softmax损失（Temporal Softmax loss）\n",
        "\n",
        "在 RNN 语言模型中，我们在每个时间步为词汇表中的每个单词生成一个分数。我们知道每个时间步的地面真实单词，因此我们使用 softmax 损失函数在每个时间步计算损失和梯度。我们将损失在时间上求和，并在小批量上对它们进行平均。\n",
        "\n",
        "然而，有一个小问题：由于我们在小批量上操作，并且不同的字幕可能具有不同的长度，因此我们在每个字幕的末尾附加 `<NULL>` 令牌，以使它们的长度相同。我们不希望这些 `<NULL>` 令牌计入损失或梯度，因此除了分数和地面真实标签之外，我们的损失函数还接受一个 `mask` 数组，告诉它哪些元素的分数计入损失。\n",
        "\n",
        "请完成 `temporal_softmax_loss` 函数。\n"
      ],
      "metadata": {
        "id": "VVG4iS6JHJnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def temporal_softmax_loss(x, y, mask, verbose=False):\n",
        "    \"\"\"\n",
        "    A temporal version of softmax loss for use in RNNs. We assume that we are\n",
        "    making predictions over a vocabulary of size V for each timestep of a\n",
        "    timeseries of length T, over a minibatch of size N. The input x gives scores\n",
        "    for all vocabulary elements at all timesteps, and y gives the indices of the\n",
        "    ground-truth element at each timestep. We use a cross-entropy loss at each\n",
        "    timestep, summing the loss over all timesteps and averaging across the\n",
        "    minibatch.\n",
        "\n",
        "    As an additional complication, we may want to ignore the model output at some\n",
        "    timesteps, since sequences of different length may have been combined into a\n",
        "    minibatch and padded with NULL tokens. The optional mask argument tells us\n",
        "    which elements should contribute to the loss.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input scores, of shape (N, T, V)\n",
        "    - y: Ground-truth indices, of shape (N, T) where each element is in the range\n",
        "         0 <= y[i, t] < V\n",
        "    - mask: Boolean array of shape (N, T) where mask[i, t] tells whether or not\n",
        "      the scores at x[i, t] should contribute to the loss.\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving loss\n",
        "    - dx: Gradient of loss with respect to scores x.\n",
        "    \"\"\"\n",
        "    dx, loss = None, None\n",
        "\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the temporal version of softmax loss.                      #\n",
        "    ##############################################################################\n",
        "    N, T, V = x.shape\n",
        "\n",
        "    # Reshape inputs for convenience\n",
        "    x_flat = x.reshape(N * T, V)\n",
        "    y_flat = y.reshape(N * T)\n",
        "    mask_flat = mask.reshape(N * T)\n",
        "\n",
        "    # Compute softmax probabilities\n",
        "    # Shift logits for numerical stability\n",
        "    probs = np.exp(x_flat)\n",
        "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
        "\n",
        "    # Loss: -log(prob of correct class), masked\n",
        "    correct_logprobs = -np.log(probs[np.arange(N * T), y_flat] + 1e-12)\n",
        "    loss = np.sum(mask_flat * correct_logprobs) / N\n",
        "\n",
        "    # Gradient\n",
        "    dx_flat = probs.copy()\n",
        "    dx_flat[np.arange(N * T), y_flat] -= 1\n",
        "    dx_flat /= N\n",
        "    dx_flat *= mask_flat[:, None]  # Only backpropagate through unmasked positions\n",
        "\n",
        "    # Reshape gradient to match original x shape\n",
        "    dx = dx_flat.reshape(N, T, V)\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "    return loss, dx"
      ],
      "metadata": {
        "id": "zdvtrqRhHNWh"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "运行以下代码对损失进行合理性检查，并对该函数进行数值梯度检查。您应该看到 dx 的误差小于 `1e-7`。"
      ],
      "metadata": {
        "id": "dNeHxVlSHzkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, T, V = 100, 1, 10\n",
        "\n",
        "def check_loss(N, T, V, p):\n",
        "    x = 0.001 * np.random.randn(N, T, V)\n",
        "    y = np.random.randint(V, size=(N, T))\n",
        "    mask = np.random.rand(N, T) <= p\n",
        "    print(temporal_softmax_loss(x, y, mask)[0])\n",
        "\n",
        "check_loss(100, 1, 10, 1.0)   # Should be about 2.3\n",
        "check_loss(100, 10, 10, 1.0)  # Should be about 23\n",
        "check_loss(5000, 10, 10, 0.1) # Should be about 2.3\n",
        "\n",
        "# Gradient check for temporal softmax loss\n",
        "N, T, V = 7, 8, 9\n",
        "\n",
        "x = np.random.randn(N, T, V)\n",
        "y = np.random.randint(V, size=(N, T))\n",
        "mask = (np.random.rand(N, T) > 0.5)\n",
        "\n",
        "loss, dx = temporal_softmax_loss(x, y, mask, verbose=False)\n",
        "\n",
        "dx_num = eval_numerical_gradient(lambda x: temporal_softmax_loss(x, y, mask)[0], x, verbose=False)\n",
        "\n",
        "print('dx error: ', rel_error(dx, dx_num))"
      ],
      "metadata": {
        "id": "PuXRPHidItBq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a42772-e25e-480c-ddf0-30bef4c6c243"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.302561846868156\n",
            "23.025873023819567\n",
            "2.250098233317948\n",
            "dx error:  1.922242943999065e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 用于图像字幕生成的RNN\n",
        "\n",
        "现在您已经实现了必要的层，可以将它们组合起来构建一个图像字幕生成模型。\n",
        "\n",
        "在 `loss` 函数中实现模型的前向和反向传播。您只需要实现 `cell_type='rnn'` 时的基本RNN情况。"
      ],
      "metadata": {
        "id": "VK6lvj9kJ1w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptioningRNN(object):\n",
        "    \"\"\"\n",
        "    A CaptioningRNN produces captions from image features using a recurrent\n",
        "    neural network.\n",
        "\n",
        "    The RNN receives input vectors of size D, has a vocab size of V, works on\n",
        "    sequences of length T, has an RNN hidden dimension of H, uses word vectors\n",
        "    of dimension W, and operates on minibatches of size N.\n",
        "\n",
        "    Note that we don't use any regularization for the CaptioningRNN.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=128,\n",
        "                 hidden_dim=128, cell_type='rnn', dtype=np.float32):\n",
        "        \"\"\"\n",
        "        Construct a new CaptioningRNN instance.\n",
        "\n",
        "        Inputs:\n",
        "        - word_to_idx: A dictionary giving the vocabulary. It contains V entries,\n",
        "          and maps each string to a unique integer in the range [0, V).\n",
        "        - input_dim: Dimension D of input image feature vectors.\n",
        "        - wordvec_dim: Dimension W of word vectors.\n",
        "        - hidden_dim: Dimension H for the hidden state of the RNN.\n",
        "        - cell_type: What type of RNN to use; either 'rnn' or 'lstm'.\n",
        "        - dtype: numpy datatype to use; use float32 for training and float64 for\n",
        "          numeric gradient checking.\n",
        "        \"\"\"\n",
        "        if cell_type not in {'rnn', 'lstm'}:\n",
        "            raise ValueError('Invalid cell_type \"%s\"' % cell_type)\n",
        "\n",
        "        self.cell_type = cell_type\n",
        "        self.dtype = dtype\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
        "        self.params = {}\n",
        "\n",
        "        vocab_size = len(word_to_idx)\n",
        "\n",
        "        self._null = word_to_idx['<NULL>']\n",
        "        self._start = word_to_idx.get('<START>', None)\n",
        "        self._end = word_to_idx.get('<END>', None)\n",
        "\n",
        "        # Initialize word vectors\n",
        "        self.params['W_embed'] = np.random.randn(vocab_size, wordvec_dim)\n",
        "        self.params['W_embed'] /= 100\n",
        "\n",
        "        # Initialize CNN -> hidden state projection parameters\n",
        "        self.params['W_proj'] = np.random.randn(input_dim, hidden_dim)\n",
        "        self.params['W_proj'] /= np.sqrt(input_dim)\n",
        "        self.params['b_proj'] = np.zeros(hidden_dim)\n",
        "\n",
        "        # Initialize parameters for the RNN\n",
        "        dim_mul = {'lstm': 4, 'rnn': 1}[cell_type]\n",
        "        self.params['Wx'] = np.random.randn(wordvec_dim, dim_mul * hidden_dim)\n",
        "        self.params['Wx'] /= np.sqrt(wordvec_dim)\n",
        "        self.params['Wh'] = np.random.randn(hidden_dim, dim_mul * hidden_dim)\n",
        "        self.params['Wh'] /= np.sqrt(hidden_dim)\n",
        "        self.params['b'] = np.zeros(dim_mul * hidden_dim)\n",
        "\n",
        "        # Initialize output to vocab weights\n",
        "        self.params['W_vocab'] = np.random.randn(hidden_dim, vocab_size)\n",
        "        self.params['W_vocab'] /= np.sqrt(hidden_dim)\n",
        "        self.params['b_vocab'] = np.zeros(vocab_size)\n",
        "\n",
        "        # Cast parameters to correct dtype\n",
        "        for k, v in self.params.items():\n",
        "            self.params[k] = v.astype(self.dtype)\n",
        "\n",
        "\n",
        "    def loss(self, features, captions):\n",
        "        \"\"\"\n",
        "        Compute training-time loss for the RNN. We input image features and\n",
        "        ground-truth captions for those images, and use an RNN (or LSTM) to compute\n",
        "        loss and gradients on all parameters.\n",
        "\n",
        "        Inputs:\n",
        "        - features: Input image features, of shape (N, D)\n",
        "        - captions: Ground-truth captions; an integer array of shape (N, T) where\n",
        "          each element is in the range 0 <= y[i, t] < V\n",
        "\n",
        "        Returns a tuple of:\n",
        "        - loss: Scalar loss\n",
        "        - grads: Dictionary of gradients parallel to self.params\n",
        "        \"\"\"\n",
        "        # Cut captions into two pieces: captions_in has everything but the last word\n",
        "        # and will be input to the RNN; captions_out has everything but the first\n",
        "        # word and this is what we will expect the RNN to generate. These are offset\n",
        "        # by one relative to each other because the RNN should produce word (t+1)\n",
        "        # after receiving word t. The first element of captions_in will be the START\n",
        "        # token, and the first element of captions_out will be the first word.\n",
        "        captions_in = captions[:, :-1]\n",
        "        captions_out = captions[:, 1:]\n",
        "\n",
        "        # You'll need this\n",
        "        mask = (captions_out != self._null)\n",
        "\n",
        "        # Weight and bias for the affine transform from image features to initial\n",
        "        # hidden state\n",
        "        W_proj, b_proj = self.params['W_proj'], self.params['b_proj']\n",
        "\n",
        "        # Word embedding matrix\n",
        "        W_embed = self.params['W_embed']\n",
        "\n",
        "        # Input-to-hidden, hidden-to-hidden, and biases for the RNN\n",
        "        Wx, Wh, b = self.params['Wx'], self.params['Wh'], self.params['b']\n",
        "\n",
        "        # Weight and bias for the hidden-to-vocab transformation.\n",
        "        W_vocab, b_vocab = self.params['W_vocab'], self.params['b_vocab']\n",
        "\n",
        "        loss, grads = 0.0, {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward and backward passes for the CaptioningRNN.   #\n",
        "        # In the forward pass you will need to do the following:                   #\n",
        "        # (1) Use an affine transformation to compute the initial hidden state     #\n",
        "        #     from the image features. This should produce an array of shape (N, H)#\n",
        "        # (2) Use a word embedding layer to transform the words in captions_in     #\n",
        "        #     from indices to vectors, giving an array of shape (N, T, W).         #\n",
        "        # (3) Use either a vanilla RNN or LSTM (depending on self.cell_type) to    #\n",
        "        #     process the sequence of input word vectors and produce hidden state  #\n",
        "        #     vectors for all timesteps, producing an array of shape (N, T, H).    #\n",
        "        # (4) Use a (temporal) affine transformation to compute scores over the    #\n",
        "        #     vocabulary at every timestep using the hidden states, giving an      #\n",
        "        #     array of shape (N, T, V).                                            #\n",
        "        # (5) Use (temporal) softmax to compute loss using captions_out, ignoring  #\n",
        "        #     the points where the output word is <NULL> using the mask above.     #\n",
        "        #                                                                          #\n",
        "        # In the backward pass you will need to compute the gradient of the loss   #\n",
        "        # with respect to all model parameters. Use the loss and grads variables   #\n",
        "        # defined above to store loss and gradients; grads[k] should give the      #\n",
        "        # gradients for self.params[k].                                            #\n",
        "        ############################################################################\n",
        "        h0 = features @ W_proj + b_proj\n",
        "\n",
        "        embed, cache_embed = word_embedding_forward(captions_in, W_embed)\n",
        "\n",
        "        h, cache_rnn = rnn_forward(embed, h0, Wx, Wh, b)\n",
        "\n",
        "        scores, cache_affine = temporal_affine_forward(h, W_vocab, b_vocab)\n",
        "\n",
        "        loss, dscores = temporal_softmax_loss(scores, captions_out, mask)\n",
        "\n",
        "        dh, dW_vocab, db_vocab = temporal_affine_backward(dscores, cache_affine)\n",
        "\n",
        "        dembed, dh0, dWx, dWh, db = rnn_backward(dh, cache_rnn)\n",
        "\n",
        "        grads['W_embed'] = word_embedding_backward(dembed, cache_embed)\n",
        "\n",
        "        dfeatures = dh0.dot(W_proj.T)\n",
        "        grads['W_proj'] = features.T.dot(dh0)\n",
        "        grads['b_proj'] = np.sum(dh0, axis=0)\n",
        "\n",
        "        grads['Wx'] = dWx\n",
        "        grads['Wh'] = dWh\n",
        "        grads['b'] = db\n",
        "\n",
        "        grads['W_vocab'] = dW_vocab\n",
        "        grads['b_vocab'] = db_vocab\n",
        "        pass\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        return loss, grads"
      ],
      "metadata": {
        "id": "c1TsHB8rKDOb"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "完成后，运行以下代码使用一个小的测试案例检查您的前向传播；您应该看到误差小于 `1e-10`。"
      ],
      "metadata": {
        "id": "bnSXZBVIKL0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, D, W, H = 10, 20, 30, 40\n",
        "word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n",
        "V = len(word_to_idx)\n",
        "T = 13\n",
        "\n",
        "model = CaptioningRNN(word_to_idx,\n",
        "          input_dim=D,\n",
        "          wordvec_dim=W,\n",
        "          hidden_dim=H,\n",
        "          cell_type='rnn',\n",
        "          dtype=np.float64)\n",
        "\n",
        "# Set all model parameters to fixed values\n",
        "for k, v in model.params.items():\n",
        "    model.params[k] = np.linspace(-1.4, 1.3, num=v.size).reshape(*v.shape)\n",
        "\n",
        "features = np.linspace(-1.5, 0.3, num=(N * D)).reshape(N, D)\n",
        "captions = (np.arange(N * T) % V).reshape(N, T)\n",
        "\n",
        "loss, grads = model.loss(features, captions)\n",
        "expected_loss = 9.83235591003\n",
        "\n",
        "print('loss: ', loss)\n",
        "print('expected loss: ', expected_loss)\n",
        "print('error: ', abs(loss - expected_loss))"
      ],
      "metadata": {
        "id": "2noSpSg-KNAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f606f7-00c9-4e26-f1f3-1f432d1727b0"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  9.832355909980949\n",
            "expected loss:  9.83235591003\n",
            "error:  4.9050541406359116e-11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "运行以下代码对 `CaptioningRNN` 类进行数值梯度检查；您应该看到误差小于 `1e-5`。"
      ],
      "metadata": {
        "id": "QOjv5bzKKXzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(231)\n",
        "\n",
        "batch_size = 2\n",
        "timesteps = 3\n",
        "input_dim = 4\n",
        "wordvec_dim = 5\n",
        "hidden_dim = 6\n",
        "word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n",
        "vocab_size = len(word_to_idx)\n",
        "\n",
        "captions = np.random.randint(vocab_size, size=(batch_size, timesteps))\n",
        "features = np.random.randn(batch_size, input_dim)\n",
        "\n",
        "model = CaptioningRNN(word_to_idx,\n",
        "          input_dim=input_dim,\n",
        "          wordvec_dim=wordvec_dim,\n",
        "          hidden_dim=hidden_dim,\n",
        "          cell_type='rnn',\n",
        "          dtype=np.float64,\n",
        "        )\n",
        "\n",
        "loss, grads = model.loss(features, captions)\n",
        "\n",
        "for param_name in sorted(grads):\n",
        "    f = lambda _: model.loss(features, captions)[0]\n",
        "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
        "    e = rel_error(param_grad_num, grads[param_name])\n",
        "    print('%s relative error: %e' % (param_name, e))"
      ],
      "metadata": {
        "id": "k0Ir6gLEKX-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98663e53-364e-45a7-b8f3-f147f47cc20d"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_embed relative error: 2.769416e-09\n",
            "W_proj relative error: 1.112417e-08\n",
            "W_vocab relative error: 6.900114e-09\n",
            "Wh relative error: 1.443682e-08\n",
            "Wx relative error: 6.191815e-07\n",
            "b relative error: 8.001363e-10\n",
            "b_proj relative error: 6.827996e-09\n",
            "b_vocab relative error: 1.690335e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 结语\n",
        "\n",
        "恭喜你！你已经完成了第二次作业。尽管这一路历经艰辛，但是你对于循环神经网络有了更加深刻的理解！\n",
        "\n",
        "\n",
        "\n",
        ">本次作业负责人：郜今（助教），gaojin@sjtu.edu.cn。\n",
        "最后请允许我再次强调，作业在 Canvas 上提交，只需要上传一份 ipynb 文件，请保留每个单元格的运行结果，注意时间节点。 如有任何问题，请联系[助教](https://cs7353.netlify.app/staff/)。"
      ],
      "metadata": {
        "id": "dWYhbqCweiHj"
      }
    }
  ]
}